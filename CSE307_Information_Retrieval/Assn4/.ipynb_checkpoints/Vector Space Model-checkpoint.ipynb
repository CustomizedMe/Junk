{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/parzival/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parzival/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/parzival/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "directory = \"../collection/20_newsgroup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing :\n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(text, lemmatizer=WordNetLemmatizer()):\n",
    "    words = [w.lower() for w in word_tokenize(text)]\n",
    "    tokens = []\n",
    "    for token in words:\n",
    "        tokens.extend(re.split('[^a-zA-Z]', token))\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in tokens if not token in stopwords.words('english')] \n",
    "    return list(filter(lambda token: len(token), token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc_path):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    with open(doc_path, encoding=\"utf8\", errors='ignore') as f:\n",
    "\n",
    "        tokens = []\n",
    "        endOfDoc=0\n",
    "        isHeader=1\n",
    "        \n",
    "        data = list(filter( lambda line: line!=\"\\n\",f.readlines()))\n",
    "        for line in data:\n",
    "            text = line\n",
    "            \n",
    "#             if isHeader==1:\n",
    "#                 parts = line.split(\": \")\n",
    "#                 try:\n",
    "#                     text = parts[1]\n",
    "#                 except IndexError as err:\n",
    "#                     print(line)\n",
    "#                     raise IndexError(err)\n",
    "#                 if str(parts[0])==\"Lines\":\n",
    "#                     isHeader=0\n",
    "            \n",
    "            tokens.extend(preprocess_line(text,lemmatizer))\n",
    "            \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Posting List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_posting(newsgroup=\"comp.graphics\"):\n",
    "    \n",
    "    doc_id = -1\n",
    "    doc_name = {}\n",
    "    post_list = {}\n",
    "    doc_len = {}\n",
    "    idf = {}\n",
    "    tf = {}\n",
    "    \n",
    "    working_dir = directory+\"/\"+newsgroup\n",
    "    for file in os.listdir(working_dir):\n",
    "        doc_id += 1\n",
    "        tf[doc_id] = {}\n",
    "        doc_len[doc_id] = 0\n",
    "        \n",
    "        filename = os.fsdecode(file)\n",
    "        doc_name[doc_id] = filename\n",
    "        \n",
    "        file_path = os.path.join(working_dir,file)\n",
    "        \n",
    "        tokens = preprocess_doc(file_path)\n",
    "        \n",
    "        doc_len[doc_id] = len(tokens)\n",
    "        \n",
    "        for token in tokens:    \n",
    "            if post_list.get(token) == None:\n",
    "                post_list[token] = [doc_id]\n",
    "                tf[doc_id][token] = 0\n",
    "                idf[token] = 1\n",
    "            elif tf[doc_id].get(token) == None:\n",
    "                post_list[token].append(doc_id)\n",
    "                tf[doc_id][token] = 0\n",
    "                idf[token] += 1\n",
    "            tf[doc_id][token] += 1\n",
    "            \n",
    "        for j in tf[doc_id].keys():\n",
    "            tf[doc_id][j] /= doc_len[doc_id]\n",
    "            \n",
    "    for i in idf.keys():\n",
    "        idf[i]=math.log10((doc_id+1)/idf[i])\n",
    "    \n",
    "    return {\"doc_names\": doc_name, \"post_list\": post_list, \"doc_lens\": doc_len, \"tf\":tf, \"idf\":idf}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = create_posting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSpaceModel:\n",
    "    def __init__(self,posting_list):\n",
    "        self.postings = posting_list\n",
    "        self.word_ids = {}\n",
    "        self.doc_vecs = self.__create_embeddings__(posting_list)\n",
    "        \n",
    "    def __create_embeddings__(self,posting_list):\n",
    "        no_docs = len(posting_list[\"doc_names\"].keys())\n",
    "        doc_vec = np.zeros((len(posting_list[\"doc_names\"]),len(posting_list[\"idf\"])))\n",
    "        cnt = 0\n",
    "        word_ids = {}\n",
    "        for w in posting_list[\"idf\"].keys():\n",
    "            word_ids[w] = cnt\n",
    "            cnt += 1\n",
    "        for i in range(no_docs):\n",
    "            for j in posting_list[\"tf\"][i].keys():\n",
    "                doc_vec[i][word_ids[j]]=posting_list[\"tf\"][i][j]*posting_list[\"idf\"][j]\n",
    "        for i in range(no_docs):\n",
    "            if norm(doc_vec[i])>0:\n",
    "                doc_vec[i]=doc_vec[i]/norm(doc_vec[i])\n",
    "        \n",
    "        self.word_ids = word_ids\n",
    "        return doc_vec\n",
    "    \n",
    "    def evaluate(self,query):\n",
    "        query_vec = self.__get_queryvec__(query)\n",
    "        no_docs = len(posting_list[\"doc_names\"].keys())\n",
    "        \n",
    "        doc_similarities = np.dot(self.doc_vecs,query_vec)\n",
    "        \n",
    "        ranked_order = []\n",
    "        \n",
    "        for i in range(no_docs):\n",
    "            ranked_order.append((doc_similarities[i][0],postings[\"doc_names\"][i]))\n",
    "        \n",
    "        ranked_order.sort()\n",
    "        return ranked_order\n",
    "            \n",
    "    def __get_queryvec__(self,query):\n",
    "        query_tokens = preprocess_line(query)\n",
    "        query_token_freq = Counter(query_tokens)\n",
    "        word_ids = self.word_ids\n",
    "        vec = np.zeros((len(postings[\"idf\"]),1))\n",
    "        \n",
    "        for token in query_tokens:\n",
    "            if token in word_ids.keys():\n",
    "                vec[word_ids[token]] = (counts[token]*postings[\"idf\"][token])/len(query_tokens)\n",
    "        \n",
    "        if norm(vec) != 0:\n",
    "            vec = vec/norm(vec)\n",
    "            \n",
    "        return vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VectorSpaceModel(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
