{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from numpy import dot\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "from math import factorial as fct\n",
    "from scipy.special import factorial\n",
    "\n",
    "from decimal import Decimal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/parzival/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/parzival/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/parzival/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../collection/20_newsgroup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(text, lemmatizer=WordNetLemmatizer()):\n",
    "    words = [w.lower() for w in word_tokenize(text)]\n",
    "    tokens = []\n",
    "    for token in words:\n",
    "        tokens.extend(re.split('[^a-zA-Z]', token))\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in tokens if not token in stopwords.words('english')] \n",
    "    return list(filter(lambda token: len(token), token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(doc_path):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    with open(doc_path, encoding=\"utf8\", errors='ignore') as f:\n",
    "\n",
    "        tokens = []\n",
    "        endOfDoc=0\n",
    "        isHeader=1\n",
    "        \n",
    "        data = list(filter( lambda line: line!=\"\\n\",f.readlines()))\n",
    "        for line in data:\n",
    "            text = line\n",
    "            tokens.extend(preprocess_line(text,lemmatizer))\n",
    "            \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Posting List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_posting(newsgroup=\"comp.graphics\"):\n",
    "    \n",
    "    doc_id = -1\n",
    "    doc_name = {}\n",
    "    post_list = {}\n",
    "    doc_len = {}\n",
    "    idf = {}\n",
    "    tf = {}\n",
    "    \n",
    "    working_dir = directory+\"/\"+newsgroup\n",
    "    for file in os.listdir(working_dir):\n",
    "        doc_id += 1\n",
    "        tf[doc_id] = {}\n",
    "        doc_len[doc_id] = 0\n",
    "        \n",
    "        filename = os.fsdecode(file)\n",
    "        doc_name[doc_id] = filename\n",
    "        \n",
    "        file_path = os.path.join(working_dir,file)\n",
    "        \n",
    "        tokens = preprocess_doc(file_path)\n",
    "        \n",
    "        doc_len[doc_id] = len(tokens)\n",
    "        \n",
    "        for token in tokens:    \n",
    "            if post_list.get(token) == None:\n",
    "                post_list[token] = [doc_id]\n",
    "                tf[doc_id][token] = 0\n",
    "                idf[token] = 1\n",
    "            elif tf[doc_id].get(token) == None:\n",
    "                post_list[token].append(doc_id)\n",
    "                tf[doc_id][token] = 0\n",
    "                idf[token] += 1\n",
    "            tf[doc_id][token] += 1\n",
    "            \n",
    "        for j in tf[doc_id].keys():\n",
    "            tf[doc_id][j] /= doc_len[doc_id]\n",
    "            \n",
    "    for i in idf.keys():\n",
    "        idf[i]=math.log10((doc_id+1)/idf[i])\n",
    "    \n",
    "    return {\"doc_names\": doc_name, \"post_list\": post_list, \"doc_lens\": doc_len, \"tf\":tf, \"idf\":idf}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = create_posting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClustering:\n",
    "    def __init__(self,posting_list):\n",
    "        self.postings = posting_list\n",
    "        self.word_ids = {}\n",
    "        self.doc_vecs = self.__create_embeddings__(posting_list)\n",
    "        \n",
    "    def __create_embeddings__(self,posting_list):\n",
    "        no_docs = len(posting_list[\"doc_names\"].keys())\n",
    "        doc_vec = np.zeros((len(posting_list[\"doc_names\"]),len(posting_list[\"idf\"])))\n",
    "        cnt = 0\n",
    "        word_ids = {}\n",
    "        for w in posting_list[\"idf\"].keys():\n",
    "            word_ids[w] = cnt\n",
    "            cnt += 1\n",
    "        for i in range(no_docs):\n",
    "            for j in posting_list[\"tf\"][i].keys():\n",
    "                doc_vec[i][word_ids[j]]=posting_list[\"tf\"][i][j]*posting_list[\"idf\"][j]\n",
    "        for i in range(no_docs):\n",
    "            if norm(doc_vec[i])>0:\n",
    "                doc_vec[i]=doc_vec[i]/norm(doc_vec[i])\n",
    "        \n",
    "        self.word_ids = word_ids\n",
    "        return doc_vec\n",
    "    \n",
    "    def __get_init_dist__(self,k):\n",
    "        init = list(np.random.randint(0,k,len(self.postings[\"doc_names\"])))\n",
    "        cnt = Counter(init)\n",
    "        keys = cnt.keys()\n",
    "        keys_missing = [ x for x in range(k) if x not in keys]\n",
    "\n",
    "        if len(keys_missing)!=0:\n",
    "            c=0\n",
    "            temp = {}\n",
    "            for i in range(len(init)):\n",
    "                if init[i] not in temp.keys():\n",
    "                    temp[init[i]] = 1\n",
    "                elif c<len(keys_missing):\n",
    "                    init[i] = keys_missing[c]\n",
    "                    c+=1\n",
    "                    temp[init[i]] = 1\n",
    "                else:\n",
    "                    temp[init[i]] += 1  \n",
    "        return init\n",
    "    \n",
    "    def __get_means__(self,cluster_x):\n",
    "        means =  np.zeros((k,len(self.postings[\"idf\"])))   \n",
    "        for cl in len(range(cluster_x)):\n",
    "            for d in cluster_x[cl]:\n",
    "                means[cl] += self.doc_vecs[d]\n",
    "            means[cl] /= len(cluster_x[cl]) \n",
    "        return means\n",
    "    \n",
    "    \n",
    "    def __get_cor_mat__(self,means):\n",
    "        cor_mat = np.zeros((k,len(self.postings[\"doc_names\"])))\n",
    "        k,docs = cor_mat.shape\n",
    "        for m in range(k):\n",
    "            for d in range(docs):\n",
    "                cor_mat[m][d] = np.linalg.norm(self.doc_vecs[d] - means[k])\n",
    "        return cor_mat\n",
    "    \n",
    "\n",
    "    def __check_equal__(self,cluster_x,cluster_y):\n",
    "        k = len(cluster_x)\n",
    "        for i in range(len(cluster_x)):\n",
    "            if Counter(cluster_x[i]) != Counter(cluster_y[i]):\n",
    "                return False    \n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def kmeans(self,k,iters=10):\n",
    "        cluster_x = [[] for x in range(k)]\n",
    "        cluster_y = [[] for y in range(k)]\n",
    "        temp = [[] for y in range(k)]\n",
    "        \n",
    "        \n",
    "        init = self.__get_init_dist__(k)\n",
    "        \n",
    "        for i in range(len(init)):\n",
    "            cluster_x[init[i]].append(i)\n",
    "            \n",
    "        means = np.zeros((k,len(self.postings[\"idf\"])))\n",
    "        cor_mat = np.zeros((k,len(self.postings[\"doc_names\"])))\n",
    "        \n",
    "        for it in range(iters):\n",
    "            \n",
    "            means = self.__get_means__(cluster_x)\n",
    "            cor_mat = self.__get_cor_mat__(means)\n",
    "            \n",
    "            mins = np.argmin(cor_mat,axis=0)\n",
    "            \n",
    "            for i in range(mins):\n",
    "                cluster_y[mins[i]].append(i)\n",
    "                \n",
    "            if !__check_equal__(cluster_x,cluster_y):\n",
    "                cluster_x = cluster_y\n",
    "                cluster_y = temp\n",
    "                continue\n",
    "            \n",
    "            break\n",
    "        \n",
    "        for m in range(k):\n",
    "            for i in range(len(cluster_y[m])):\n",
    "                cluster_y[m][i] = self.postings[\"doc_names\"][cluster_y[m][i]]\n",
    "        \n",
    "        return cluster_y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = VectorSpaceModel(postings)lm = LanguageModel(postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.evaluate(\"computer graphics is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
